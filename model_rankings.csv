model,total_rank,model_parameters,teaching_effectiveness_rank,tokens_per_second_rank,tokens_per_joule_rank,energy_consumption_rank,time_to_first_token_rank,avg_iops_rank,teaching_effectiveness_raw,tokens_per_second_raw,tokens_per_joule_raw,energy_consumption_raw,time_to_first_token_raw,avg_iops_raw
qwen3:0.6b,24,"0,6B",17,2,1,1,2,1,6.8,200.88,0.0,0.0,0.55,0.0
gemma3:270m,33,1B,21,3,2,2,3,2,5.5,183.02,0.0,0.0,0.55,0.0
gemma3:1b,36,1B,1,5,4,4,18,4,8.33,127.08,0.0,0.0,0.93,0.0
tinyllama:latest,38,"1,1B",22,1,3,3,6,3,5.0,209.23,0.0,0.0,0.6,0.0
falcon3:1b,40,1B,4,8,7,7,7,7,7.8,103.65,0.0,0.0,0.66,0.0
llama3.2:1b,48,1B,16,7,5,5,10,5,7.0,117.53,0.0,0.0,0.75,0.0
exaone-deep:2.4b,49,"2,7B",6,9,10,10,4,10,7.78,97.32,0.0,0.0,0.55,0.0
qwen3:1.7b,52,"1,7B",8,6,9,9,11,9,7.7,119.28,0.0,0.0,0.75,0.0
granite4:1b,61,1B,7,17,6,6,19,6,7.7,54.36,0.0,0.0,1.04,0.0
deepseek-r1:1.5b,63,"1,5B",20,4,8,8,15,8,5.6,127.93,0.0,0.0,0.83,0.0
granite4:3b,67,3B,12,14,12,12,5,12,7.5,67.34,0.0,0.0,0.58,0.0
llama3.2:3b,69,3B,10,12,11,11,14,11,7.6,69.89,0.0,0.0,0.8,0.0
phi3:3.8b,69,"3,8B",13,10,15,15,1,15,7.5,78.8,0.0,0.0,0.45,0.0
cogito:3b,79,3B,18,13,13,13,9,13,6.8,68.56,0.0,0.0,0.71,0.0
falcon3:3b,79,3B,14,11,14,14,12,14,7.4,75.0,0.0,0.0,0.77,0.0
qwen3:4b,79,4B,2,15,18,18,8,18,8.1,62.87,0.0,0.0,0.68,0.0
phi4-mini-reasoning:3.8b,95,"3,8B",11,16,16,16,20,16,7.6,55.28,0.0,0.0,1.11,0.0
mistral:7b,95,7B,3,19,20,20,13,20,8.0,41.61,0.0,0.0,0.78,0.0
gemma3:4b,104,4B,9,22,17,17,22,17,7.7,28.61,0.0,0.0,1.37,0.0
nemotron-mini:4b,110,4B,19,18,19,19,16,19,6.3,54.13,0.0,0.0,0.83,0.0
cogito:8b,113,8B,5,21,22,22,21,22,7.8,36.92,0.0,0.0,1.12,0.0
deepseek-r1:7b,115,7B,15,20,21,21,17,21,7.4,40.26,0.0,0.0,0.91,0.0
